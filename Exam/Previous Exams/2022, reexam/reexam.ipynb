{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Programming and Numerical Analysis\n",
    "# Re-exam 2022 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from math import fabs\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import time\n",
    "from types import SimpleNamespace\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Maximum Simulated Likelihood\n",
    "\n",
    "In this exercise, we will consider the logit model for a binary discrete choice. Just for exposition, our binary choice will be to study programming or not. \n",
    "\n",
    "The benefit of choosing to study programming is described by a linear utility index $y_i^*$. If $y_i^* > 0$ then an individual will choose to study. \n",
    "\n",
    "In our context, $y_i^*$ depends only on one background variable $x_i$. In addition, we will allow all individuals have their own idiosyncratic effect of $x_i$ on their study propensity. This is obtained by letting the coefficient on $x_i$ be comprised of a common effect, denoted $\\beta^{1}$, and an idiosyncratic part $w_i$.  \n",
    "\n",
    "The utility of studying also depends on a random shock, $\\varepsilon$, which follows a logistic distribution. \n",
    "\n",
    "We write the utility index as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\ty_{i}^*  =&\\beta^{0} + (\\beta^{1} + w_i)x_{i} + \\varepsilon_i \\\\\n",
    "    & \\varepsilon_i \\sim logistic(0,1) \\\\\n",
    "\t& w_i \\sim \\mathcal{N}(0,\\sigma_{w}^{2})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that the econometrician cannot observe the utility index. One can only observe *actual choices* which are based on the underlying index. We therefore associate the indicator variable $y_i$ with the choice taken by individual $i$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\ty_i & = 1 \\Leftrightarrow y^* > 0 \\Leftrightarrow \\text{Study programming} \\\\\n",
    "\ty_i & = 0 \\Leftrightarrow y^* \\le 0 \\Leftrightarrow \\text{Do not study programming}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Because we assume that the utility shocks follow a logistic distribution, we can formulate the **probability** that an individual chooses to take the action by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\tP(y_i = 1|x_i, w_i;\\beta) &= \\frac{\\exp(\\beta^{0} + (\\beta^{1} + w_i)x_{i})}{1 + \\exp(\\beta^{0} + (\\beta^{1} + w_i)x_{i})} \\equiv \\Lambda(\\beta^{0} + (\\beta^{1} + w_i)x_{i}) \\\\\n",
    "\tP(y_i = 0|x_i, w_i;\\beta) &= 1 - \\Lambda(\\beta^{0} + (\\beta^{1} + w_i)x_{i})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**CHALLENGE**  \n",
    "It would be easy to proceed with estimation if $w_i$ was observed, then it would just enter as data. Yet it is not, and so we need to take it into account in some other way. The standard approach is to take the **expectation** of the density of $y_i$ over $w_i$ for each individual and use this expected density for likelihood contributions.   \n",
    "\n",
    "To this end, you can use **Monte Carlo integration**. That implies taking $N^d$ draws of $w$ for each individual, plugging them into the density of $y_i$, and then averaging. In that way, the **approximated** density of $y_i$, denoted $f(y_i|x_i,\\beta^{0},\\beta^{1},\\sigma_w)$, can be written as \n",
    "$$\n",
    "\tf(y_i|x_i,\\beta^{0},\\beta^{1},\\sigma_w) = \\frac{1}{N^d}\\sum \\limits_{k=1}^{N^d} y_i\\Lambda(\\beta^{0} + (\\beta^{1} + w_k)x_{i}) + (1-y_i)[1-\\Lambda(\\beta^{0} + (\\beta^{1} + w_k)x_{i})]    \n",
    "$$\n",
    "Which means that the log-likelihood of the model can be written as\n",
    "$$\n",
    "\t\\text{log-likelihood}(\\beta^{0},\\beta^{1},\\sigma_w) = \\sum \\limits_{i=1}^{N} \\log \\left\\{\\frac{1}{N^d}\\sum \\limits_{k=1}^{N^d} y_i\\Lambda(\\beta^{0} + (\\beta^{1} + w_k)x_{i}) + (1-y_i)[1-\\Lambda(\\beta^{0} + (\\beta^{1} + w_k)x_{i})] \\right\\}   \n",
    "$$\n",
    "\n",
    "By maximizing $\\text{log-likelihood}(\\beta^{0},\\beta^{1},\\sigma_w)$, you will get an estimate of $[\\beta^0, \\beta^1, \\sigma_w]$.\n",
    "\n",
    "**Important tricks**  \n",
    "1. Use that $w_k = \\sigma_w Z_k$ whenever $Z_k$ is a standard normal random variable and $\\mu_w=0$.  \n",
    "2. You need to keep the draws of $Z$ **fixed** during estimation to avoid unnecessary noise (*so that only $\\sigma_w$ changes while optimizing*).  \n",
    "   You may as well use the same set of draws across individuals. They are therefore included in `mp` below.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = SimpleNamespace()\n",
    "\n",
    "# Parameters\n",
    "mp.theta = np.array([1.0, 1.0, 1.0])                # Parameters in data generating process: [beta^0, beta^1, sigma_w]\n",
    "mp.theta_names = ['beta_0', 'beta_1', 'sigma_w']    #\n",
    "mp.N = 10000                                        # Number of observed individuals\n",
    "mp.Ndraws = 500                                     # Number of simulation draws for each individual\n",
    "mp.Z = np.random.normal(size=(1,mp.Ndraws))         # Draws from a standard normal distribution \n",
    "\n",
    "# Fix seed \n",
    "np.random.seed(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `DGP(mp)` below delivers the data generating process, $N$ observations of $(y_i,x_i)$.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DGP(mp):\n",
    "    ''' The data generating process behind binary choice model\n",
    "    \n",
    "    Args:\n",
    "        mp (SimpleNamespace): object containing parameters for data generation\n",
    "    \n",
    "    Returns:\n",
    "        y_obs (ndarray): indicator for binary choices made by individuals\n",
    "        x_obs (ndarray): independent variables \n",
    "    \n",
    "    '''\n",
    "\n",
    "    # a. Draw data points \n",
    "    x = np.random.normal(loc=0.0,scale=2.0,size=(mp.N,1))           # Explanatory variable\n",
    "    e = np.random.logistic(loc=0.0,scale=1.0,size=(mp.N,1))         # Logistic taste shocks\n",
    "    w = np.random.normal(loc=0.0,scale=mp.theta[2],size=(mp.N, 1))  # Individual effects of x on study propensity\n",
    "    ystar = mp.theta[0] + (mp.theta[1] + w)*x + e                   # The utility index of choosing alternative 1 \n",
    "    y = (ystar > 0)                                                 # Choose alternative 1 if utility index is positive\n",
    "\n",
    "    return y, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data \n",
    "y_obs, x_obs = DGP(mp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 1**  \n",
    "Implement the function `log_likelihood(theta, x_obs, y_obs, mp)` in the cell below, based on the description above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x_obs, y_obs, mp):\n",
    "    ''' Computes the sum of log-likelihood contributions for observations given theta\n",
    "    \n",
    "    Args:\n",
    "        \n",
    "        theta (ndarray): coefficients in model\n",
    "        x_obs (ndarray): independent variables\n",
    "        y_obs (ndarray): observed binary choices\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        (float): sum of log-likelihood contributions  \n",
    "    '''\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 2**  \n",
    "Create 3 plots in which you profile the log-likelihood for *one paramater at a time* - while keeping the others fixed at the data generating values in `mp.theta`.   \n",
    "For instance, let $\\beta^0$ vary over the interval $[0.5,1.5]$, while $\\beta^1,\\sigma_w$ are unchanged.  \n",
    "Verify that the log-likelihood peaks at the data-generating values of the paramaters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 3**  \n",
    "Estimate $\\beta^0,\\beta^1,\\sigma_w$ by maximizing the log-likelihood function.  \n",
    "You can use the BFGS method in `scipy` with numerical derivatives or Nealder-Mead.  \n",
    "Use the starting values `theta0 = [0.8, 1.2, 0.7]`.  \n",
    "If the optimization takes too long on your computer, then reduce $N,N^d$ to smaller numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Equilibrium on social media platforms\n",
    "\n",
    "In this exercise you will find the equilibrium distribution of people on online social media platforms using a fixed point iteration procedure. \n",
    "\n",
    "The general idea is the following:  \n",
    "\n",
    "There are 3 different social media platforms: _facebook, Instagram, TikTok_ and 2 kinds of users: _young_, _old_.  \n",
    "**Young** users make up a fraction $\\rho^{y}$ of the total population and **old** users make up the fraction $\\rho^{o} = 1-\\rho^{y}$. \n",
    "\n",
    "Each user type derives utility from being on a given platform through **3 channels**: \n",
    "   1. A type specific **valuation** of the platform, $w$.   \n",
    "      This captures how much one likes the interface, features, etc.\n",
    "   2. The **share** of the population who is on the platform, denoted $s$.   \n",
    "      This is a network effect, capturing that more users makes a network more interesting.\n",
    "   3. A stochastic taste shock, $\\varepsilon$. This follows an Extreme Value type I distribution.   \n",
    "\n",
    "Taken together, the **non-stochastic** utility a type $t \\in \\{y,o\\}$ gets from being on media platform $m \\in \\{facebook, \\: Instagram, \\: TikTok\\}$ is defined by\n",
    "$$\n",
    "u_{t,m} = w_{t,m} + s_{m}\n",
    "$$\n",
    "and total utility is given by \n",
    "$$\n",
    "V_{t,m} = u_{t,m} + \\varepsilon_{t,m}\n",
    "$$\n",
    "**OBJECTIVE**  \n",
    "The objective is to compute the _equilibrium shares_ of young users and old users being active on each of the social media platforms.  \n",
    "This gets complicated because the utility from using a platform is dependent on the choices of other users. \n",
    "\n",
    "**CHOICE PROBABILITIES**   \n",
    "To make things simple, we assume that users are only active on one of the social media platforms and everyone chooses a platform.  \n",
    "\n",
    "The fact that $\\varepsilon_{t,p}$ follows an EV I distribution (also called Gumbel) means that the **probability** of choosing each social media platform is related to the non-stochastic utility from that choice through the convenient logit form:\n",
    "$$\n",
    "P_{t,m} = \\frac{\\exp(u_{t,m})}{{\\sum_{l=1}^3 \\exp(u_{t,l})}}\n",
    "$$\n",
    "\n",
    "Therefore the **total share** of users on a platform, e.g. TikTok, can be written as \n",
    "$$\n",
    "s_{TikTok} = \\rho^{y} \\times P_{y,TikTok} + \\rho^{o} \\times P_{o,TikTok}\n",
    "$$\n",
    "\n",
    "**SOLUTION METHOD**    \n",
    "One way to find the equilibrium distribution of users on platforms is to rely on what is called succesive approximations (or fixed point iterations).  \n",
    "\n",
    "Recall that an equilibrium means that _no agent has an incentive to alter behavior_. In our context, this notion implies the following: \n",
    "1. Assume we have the **equilibrium** vector of user shares $s^*=$ [$s_1,s_2,s_3$] \n",
    "2. Then we can compute equilibrium utilities $u^*_{t,m}$ for each type and platform.\n",
    "3. Using $u^*_{t,m}$, we can compute equilibrium choice probalities $P^*_{t,m}$.\n",
    "4. Aggregating $P^*_{t,m}$ for each platform over types will deliver **the same** $s^*$ as in bullit point 1.\n",
    "\n",
    "This is called a fixed point of the model. \n",
    "\n",
    "**Only problem:** We do not know $s^*$ up front!  \n",
    "**Solution to problem:** Succesive approximations. Given some starting vector $s_0$, we can keep iterating on the problem using the ideas above until $s_k$ stops changing.   \n",
    "\n",
    "**Succesive approximation algorithm**  \n",
    "1. Define an initial guess of $s^0 = [s_1^0,s_2^0,s_3^0]$,  \n",
    "   set a convergence criterion $\\epsilon>0$,  \n",
    "   set a counter $k=0$ and a maximum number of iterations $N$.\n",
    "2. Compute $u_{t,m}^k$ for $\\forall t,m$ based on $s^k$.\n",
    "3. Compute $P^k_{t,m}$ for $\\forall t,m$ based on $u_{t,m}^k$.\n",
    "4. Compute the vector $s^{k+1}$ using all $P^k_{t,m}$ and population shares [$\\rho^y,\\rho^0]$.\n",
    "5. Compute $u_{t,m}^{k+1}$ for $\\forall t,m$ using $s^{k+1}$.\n",
    "6. If $\\text{max}_{t,m}|u_{t,m}^{k+1}-u_{t,m}^{k}| < \\epsilon$ or $k>N$ then stop.  \n",
    "   Otherwise:  \n",
    "   set $k = k + 1$ and repeat from step 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "mp = SimpleNamespace()\n",
    "mp.platforms = ['facebook', 'instagram', 'tiktok'] # Order of platforms\n",
    "mp.rho_y = 0.6                                     # Share of young users\n",
    "mp.rho_o = 1.0-mp.rho_y                            # Share of old users\n",
    "mp.w_y = np.array([0.7, 1.5, 1.8])                 # Valuations by young users\n",
    "mp.w_o = np.array([0.9, 0.2, 0.01])                # Valuations by old users\n",
    "mp.eps = 1e-8                                      # Convergence criterion\n",
    "mp.N = 100                                         # Max number of iterations\n",
    "s0 = np.array([0.9, 0.05, 0.05])                   # Initial guess on shares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 1**  \n",
    "Find the equilibrium distribution of users on social media platforms using succesive approximation method above and the parameters in `mp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that implements succesive approximations on social media usage\n",
    "def some_eq(mp, s0):\n",
    "    ''' Find user distribution on social media platforms by succesive approximations\n",
    "\n",
    "    Args:\n",
    "        mp (SimpleNamespace): model parameters\n",
    "        s0 (ndarray): initial vector of user shares for platforms\n",
    "\n",
    "    Returns:\n",
    "        (ndarray): equilibrium share of each user type on each platform\n",
    "    '''\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we change the preferences of young users by letting their utility be **negatively** affected by the share of old users on a platform.  \n",
    "Thus, the utility of young users is given by \n",
    "$$\n",
    "u^y_{t,m} = w_{t,m} + s_{m}^{y} - s_{m}^o\n",
    "$$ \n",
    "where \n",
    "$$\n",
    "s_{m}^t = \\rho^{t} \\times P_{t,m}\n",
    "$$\n",
    "while the utility of old users is the same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 2**  \n",
    "Find the new equilibrium distribution of users on the platforms now that young users do not like hanging out with the old. Comment on the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. A convergence safe hybrid rootfinder.\n",
    "\n",
    "We will now consider a rootfinding algorithm that synthezises the bisection method (B) and Newton-Raphson (NR).  \n",
    "\n",
    "Each algorithm has its pros and cons. Bisection has ensured convergence, yet it might require many steps before that happens. The Newton-Raphson will normally converge in much fewer steps, but there is a chance that it will instead diverge if the domain of the function is bounded and thus never find the root.   \n",
    "\n",
    "The hybrid algorithm combines the two methods to make sure that convergence is guaranteed at a faster rate than bisection can deliver alone. \n",
    "\n",
    "We are considering a function $f$ that lives on a bounded domain $[a,b]$.  \n",
    "The idea is to primarily take a Newton step as default, but then use bisection if this step jumps out of bounds of the function domain. \n",
    "\n",
    "Recall that **Newton-Raphson** will find the root, $x^*$, of a function $f(x)$ by taking iterative steps on the form:\n",
    "$$\n",
    "x_{k+1} = x_{k} - \\frac{f(x_{k})}{f^{\\prime}(x_{k})} \\equiv \\mathcal{N}(x_k) \n",
    "$$\n",
    "\n",
    "The **Bisection** method instead takes the midpoint on the interval that the root must reside on and assigns $x_{k+1}$ to that. We can write a bisection step as:\n",
    "$$\n",
    "x_{k+1} = a_k + \\frac{b_k-a_k}{2} \\equiv \\mathcal{B}(a_{k}, b_{k}) \\\\\n",
    "$$ \n",
    "\n",
    "In addition, the bisection algorithm must update the upper and lower bound on the interval on which the root must lie. It does so by eliminating the half part of the current interval which does **not** include the new midpoint. \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{C}: \\textbf{if} \\:\\: f(a_k) \\times f(x_{k+1}) < 0 \\:\\: & \\textbf{then}  \\:\\: & b_{k+1} = x_{k+1}, \\:\\: a_{k+1} = a_{k} \\\\  \n",
    "& \\textbf{else}  \\:\\: & a_{k+1} = x_{k+1}, \\:\\: b_{k+1} = b_{k}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Note:** we can safely apply this elimation of the half-interval at **each iteration**, not just when using bisection, since we know that the discarded half will not contain the root.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Algorithm: `safe_NR`**\n",
    "1. Provide the following parameters: \n",
    "   * $\\epsilon > 0$ (the convergence criterion)\n",
    "   * $x_0$ (the initial **guess** on $x^*$). \n",
    "   * [$a_0$, $b_0$] (the interval on which $x^*$ should reside).\n",
    "   * set $k=0$. \n",
    "2. Compute $x_1 = \\mathcal{N}(x_0)$\n",
    "3. If $x_1 \\notin [a_0, b_0]$ use $x_1 = \\mathcal{B}(x_0)$\n",
    "4. Update the bounds $a_0$ and $b_0$ according to condition $\\mathcal{C}$\n",
    "5. If $|f(x_1)|<\\epsilon$ then stop.  \n",
    "   Otherwise, set $k=k+1$ and reiterate from point 2.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "f = lambda x: np.arctan(x)      # function f\n",
    "fprime = lambda x : 1/(1+x**2)  # derivative of f\n",
    "a = -4.0                        # lower bound on domain\n",
    "b = 4.0                         # upper bound on domain\n",
    "x0 = 1.5                        # initial guess on root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercises**  \n",
    "1. Plot the function $\\arctan(x)$ on the domain $[a,b]$ which is specified above.  \n",
    "   Comment on what the problem with a pure Newton-Raphson method might be in this instance. \n",
    "2. Implement `safe_NR` in code and find $x^*$ when using $x_0=1.5$ as initial guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_NR(f, fprime, a, b, x0, maxit=100, tol=1e-14):\n",
    "    \"\"\" Root of f(x) obtained by switching between Newton-Raphson and bisection.\n",
    "    \n",
    "    Args:\n",
    "        f (function): function to find root of\n",
    "        fprime (function): derivative of f\n",
    "        a (float): lower bound\n",
    "        b (float): upper bound\n",
    "        x0 (float): initial guess of root\n",
    "        maxit (int): maximum number of iterations\n",
    "        tol (float): tolerance\n",
    "    \"\"\"   \n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee968de5d47d6b918af8f2f18334bb3481e4ba075c65afd32ad8e51b4578d678"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
